{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science\n",
    "## Homework 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name:  Harish Puvvada\n",
    "\n",
    "Student Netid:  hp1047\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will be looking at data generated by particle physicists to test whether machine learning can help classify whether certain particle decay experiments identify the presence of a Higgs Boson. One does not need to know anything about particle physics to do well here, but if you are curious, full feature and data descriptions can be found here:\n",
    "\n",
    "- https://www.kaggle.com/c/higgs-boson/data\n",
    "- http://higgsml.lal.in2p3.fr/files/2014/04/documentation_v1.8.pdf\n",
    "\n",
    "The goal of this assignment is to learn to use cross-validation for model selection as well as bootstrapping for error estimation. Weâ€™ll also use learning curve analysis to understand how well different algorithms make use of limited data. For more documentation on cross-validation with Python, you can consult the following:\n",
    "\n",
    "- http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Data preparation (5 points)\n",
    "Create a data preparation and cleaning function that does the following:\n",
    "- Has a single input that is a file name string\n",
    "- Reads data (the data is comma separated, has a row header and the first column `EventID` is the index) into a pandas `dataframe`\n",
    "- Cleans the data\n",
    "  - Convert the feature `Label` to numeric (choose the minority class to be equal to 1)\n",
    "    - Create a feature `Y` with numeric label\n",
    "    - Drop the feature `Label`\n",
    "  - If a feature has missing values (i.e., `-999`): \n",
    "    - Create a dummy variable for the missing value\n",
    "      - Call the variable `orig_var_name` + `_mv` where `orig_var_name` is the name of the actual var with a missing value\n",
    "      - Give this new variable a 1 if the original variable is missing\n",
    "    - Replace the missing value with the average of the feature (make sure to compute the mean on records where the value isn't missing). You may find pandas' `.replace()` function useful.\n",
    "- After the above is done, rescales the data so that each feature has zero mean and unit variance (hint: look up sklearn.preprocessing)\n",
    "- Returns the cleaned and rescaled dataset\n",
    "\n",
    "Hint: as a guide, this function can easily be done in less than 15 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "        DER_mass_MMC DER_mass_transverse_met_lep DER_mass_vis DER_pt_h  \\\n",
      "EventId                                                                  \n",
      "100000        138.47                      51.655       97.827    27.98   \n",
      "100001       160.937                      68.768      103.235   48.146   \n",
      "100002       79.9023                     162.172      125.953   35.635   \n",
      "100003       143.905                      81.417       80.943    0.414   \n",
      "100004       175.864                      16.915      134.805   16.405   \n",
      "\n",
      "        DER_deltaeta_jet_jet DER_mass_jet_jet DER_prodeta_jet_jet  \\\n",
      "EventId                                                             \n",
      "100000                  0.91          124.711               2.666   \n",
      "100001               2.38366         -716.344             -615.26   \n",
      "100002               18.7588          2.38366            -716.344   \n",
      "100003              0.333167          18.7588             2.38366   \n",
      "100004              -38.5244         0.333167             18.7588   \n",
      "\n",
      "        DER_deltar_tau_lep DER_pt_tot  Y DER_mass_MMC_mv  \\\n",
      "EventId                                                    \n",
      "100000               3.064     41.928  1            none   \n",
      "100001               3.473      2.078  0            none   \n",
      "100002               3.148      9.336  0            Long   \n",
      "100003                3.31      0.414  0            none   \n",
      "100004               3.891     16.405  0            none   \n",
      "\n",
      "        DER_mass_transverse_met_lep_mv DER_mass_vis_mv DER_pt_h_mv  \\\n",
      "EventId                                                              \n",
      "100000                            none            none        none   \n",
      "100001                            none            none        none   \n",
      "100002                            none            none        none   \n",
      "100003                            none            none        none   \n",
      "100004                            none            none        none   \n",
      "\n",
      "        DER_deltaeta_jet_jet_mv DER_mass_jet_jet_mv DER_prodeta_jet_jet_mv  \\\n",
      "EventId                                                                      \n",
      "100000                     none                none                   none   \n",
      "100001                     Long                Long                   Long   \n",
      "100002                     Long                Long                   Long   \n",
      "100003                     Long                Long                   Long   \n",
      "100004                     Long                Long                   Long   \n",
      "\n",
      "        DER_deltar_tau_lep_mv DER_pt_tot_mv  Y_mv  \n",
      "EventId                                            \n",
      "100000                   none          none  none  \n",
      "100001                   none          none  none  \n",
      "100002                   none          none  none  \n",
      "100003                   none          none  none  \n",
      "100004                   none          none  none  \n"
     ]
    }
   ],
   "source": [
    "data_read = pd.read_csv(\"boson_training_cut_2000.csv\")\n",
    "data_EventIndexed = data_read.set_index(['EventId'])\n",
    "data_EventIndexed['Label'] = data_EventIndexed['Label'].map({'s': 1, 'b': 0})\n",
    "data_newFeature = data_EventIndexed.rename(columns = {'Label':'Y'})\n",
    "count = 0\n",
    "for col in data_newFeature.columns:\n",
    "    count = count +1 \n",
    "    data_newFeature[col+\"_mv\"] = [\"Long\" if ele  == -999 else \"none\" for ele in data_newFeature[col]]\n",
    "dataset_NaN = data_newFeature.replace([-999], [data_newFeature.mean()])\n",
    "print(\"---------------------------------------------\")\n",
    "print(dataset_NaN.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def replace(dataset):\n",
    "    count = 0\n",
    "    for col in dataset.columns:\n",
    "        count = count +1 \n",
    "        dataset[col+\"_mv\"] = [1 if ele  == -999 else 0 for ele in dataset[col]]\n",
    "    dataset_rep = dataset.replace([-999], [dataset.mean()])\n",
    "    return dataset_rep\n",
    "    \n",
    "def cleanBosonData(infile_name):\n",
    "    data_read = pd.read_csv(infile_name)\n",
    "    data_EventIndexed = data_read.set_index(['EventId'])\n",
    "    #data_EventIndexed['Label'].value_counts() - used this to decide ('s','b') to be ('1','0')\n",
    "    data_EventIndexed['Label'] = data_EventIndexed['Label'].map({'s': 1, 'b': 0})\n",
    "    data_newFeature = data_EventIndexed.rename(columns = {'Label':'Y'})\n",
    "    data_clean = replace(data_newFeature)\n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Basic evaluations (5 points)\n",
    "In this part you will build an out-of-the box logistic regression (LR) model and support vector machine (SVM). You will then plot ROC for the LR and SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Clean the two data files included in this assignment (`data/boson_training_cut_2000.csv` and `data/boson_testing_cut.csv`) and use them as training and testing data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = cleanBosonData(\"boson_training_cut_2000.csv\")\n",
    "data_test = cleanBosonData(\"boson_testing_cut.csv\")\n",
    "# print(data_test.head())\n",
    "# print(data_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. On the training set, build the following models:\n",
    "\n",
    "- A logistic regression using sklearn's `linear_model.LogisticRegression()`. For this model, use `C=1e30`.\n",
    "- An SVM using sklearn's `svm.svc()`. For this model, specify that `kernel=\"linear\"`.\n",
    "\n",
    "For each model above, plot the ROC curve of both models on the same plot. Make sure to use the test set for computing and plotting. In the legend, also print out the Area Under the ROC (AUC) for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import linear_model,svm\n",
    "clf_LR = linear_model.LogisticRegression(C=1e30)\n",
    "clf_svm = svm.SVC(kernel='linear')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Which of the two models is generally better at ranking the test set? Are there any classification thresholds where the model identified above as \"better\" would underperform the other in a classification metric (such as TPR)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Model selection with cross-validation (10 points)\n",
    "We think we might be able to improve the performance of the SVM if we perform a grid search on the hyper-parameter $C$.  Because we only have 1000 instances, we will have to use cross-validation to find the optimal $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Write a cross-validation function that does the following:\n",
    "- Takes as inputs a dataset, a label name, # of splits/folds (`k`), a sequence of values for $C$ (`cs`)\n",
    "- Performs two loops\n",
    "  - Outer Loop: `for each f in range(k)`:\n",
    "    - Splits the data into `data_train` & `data_validate` according to cross-validation logic\n",
    "  - Inner Loop: `for each c in cs`:\n",
    "    - Trains an SVM on training split with `C=c, kernel=\"linear\"`\n",
    "    - Computes AUC_c_k on validation data\n",
    "    - Stores AUC_c_k in a  dictionary of values\n",
    "- Returns a dictionary, where each key-value pair is: `c:[auc-c1,auc-c2,..auc-ck]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here\n",
    "def xValSVM(dataset, label_name, k, cs):\n",
    "    \n",
    "    return aucs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Using the function written above, do the following:\n",
    "- Generate a sequence of 10 $C$ values in the interval `[10^(-8), ..., 10^1]` (i.e., do all powers of 10 from -8 to 1).\n",
    "2.\tCall aucs = xValSVM(train, â€˜Yâ€™, 10, cs)\n",
    "3.\tFor each c in cs, get mean(AUC) and StdErr(AUC) \n",
    "4.\tCompute the value for max(meanAUC-StdErr(AUC)) across all values of c.\n",
    "5.\tGenerate a plot with the following:\n",
    "a.\tLog10(c) on the x-axis\n",
    "b.\t1 series with mean(AUC) for each c\n",
    "c.\t1 series with mean(AUC)-stderr(AUC) for each c (use â€˜k+â€™ as color pattern)\n",
    "d.\t1 series with mean(AUC)+stderr(AUC) for each c (use â€˜k--â€˜ as color pattern)\n",
    "e.\ta reference line for max(AUC-StdErr(AUC)) (use â€˜râ€™ as color pattern)\n",
    "\n",
    "Then answer the question: Did the model parameters selected beat the out-of-the-box model for SVM? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Part 4: Learning Curve with Bootstrapping\n",
    "In this HW we are trying to find the best linear model to predict if a record represents the Higgs Boson. One of the drivers of the performance of a model is the sample size of the training set.  As a data scientist, sometimes you have to decide if you have enough data or if you should invest in more.  We can use learning curve analysis to determine if we have reached a performance plateau. This will inform us on whether or not we should invest in more data (in this case it would be by running more experiments).\n",
    "\n",
    "Given a training set of size $N$, we test the performance of a model trained on a subsample of size $N_i$, where $N_i<=N$.  We can plot how performance grows as we move $N_i$ from $0$ to $N$.  \n",
    "\n",
    "Because of the inherent randomness of subsamples of size $N_i$, we should expect that any single sample of size $N_i$ might not be representative of an algorithmâ€™s performance at a given training set size. To quantify this variance and get a better generalization, we will also use bootstrap analysis. In bootstrap analysis, we pull multiple samples of size $N_i$, build a model, evaluate on a test set, and then take an average and standard error of the results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Create a bootstrap function that can do the following:\n",
    "\n",
    "def modBootstrapper(train, test, nruns, sampsize, lr, c):\n",
    "\n",
    "-\tTakes as input:\n",
    "    -\tA master training file (train)\n",
    "    -\tA master testing file (test)\n",
    "    -\tNumber of bootstrap iterations (nruns) \n",
    "    -\tSize of a bootstrap sample (sampsize)\n",
    "    -\tAn indicator variable to specific LR or SVM (lr=1)\n",
    "    -\tA c option (only applicable to SVM)\n",
    "\n",
    "-\tRuns a loop with (nruns) iterations, and within each loop:\n",
    "    -\tSample (sampsize) instances from train, with replacement\n",
    "    -\tFit either an SVM or LR (depending on options specified). For SVM, use the value of C identified using the 1 standard error method from part 3.    \n",
    "    -\tComputes AUC on test data using predictions from model in above step\n",
    "    -\tStores the AUC in a list\n",
    "\n",
    "-\tReturns the mean(AUC) and Standard Error(mean(AUC)) across all bootstrap samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here\n",
    "def modBootstrapper(train, test, nruns, sampsize, lr, c):\n",
    "    \n",
    "    return aucs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. For both LR and SVM, run 20 bootstrap samples for each samplesize in the following list: samplesizes = [50, 100, 200, 500, 1000, 1500, 2000]. (Note, this might take 10-15 mins â€¦ feel free to go grab a drink or watch Youtube while this runs).\n",
    "\n",
    "Generate a plot with the following:\n",
    "-\tLog2(samplesize) on the x-axis\n",
    "-\t2 sets of results lines, one for LR and one for SVM, the set should include\n",
    "    -\t1 series with mean(AUC) for each sampsize (use the color options â€˜gâ€™ for svm, â€˜râ€™ for lr)\n",
    "    -\t1 series with mean(AUC)-stderr(AUC) for each c (use â€˜+â€™ as color pattern, â€˜gâ€™,â€™râ€™ for SVM, LR respectively)\n",
    "    -\t1 series with mean(AUC)+stderr(AUC) for each c (use â€˜--â€˜ as color pattern â€˜gâ€™,â€™râ€™ for SVM, LR respectively)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Which of the two algorithms are more suitable for smaller sample sizes, given the set of features? If it costs twice the investment to run enough experiments to double the data, do you think it is a worthy investment?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Is there a reason why cross-validation might be biased? If so, in what direction is it biased?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
