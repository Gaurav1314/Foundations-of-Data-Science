{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science\n",
    "## Homework 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name:  Harish Puvvada\n",
    "\n",
    "Student Netid:  hp1047\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will be looking at data generated by particle physicists to test whether machine learning can help classify whether certain particle decay experiments identify the presence of a Higgs Boson. One does not need to know anything about particle physics to do well here, but if you are curious, full feature and data descriptions can be found here:\n",
    "\n",
    "- https://www.kaggle.com/c/higgs-boson/data\n",
    "- http://higgsml.lal.in2p3.fr/files/2014/04/documentation_v1.8.pdf\n",
    "\n",
    "The goal of this assignment is to learn to use cross-validation for model selection as well as bootstrapping for error estimation. Weâ€™ll also use learning curve analysis to understand how well different algorithms make use of limited data. For more documentation on cross-validation with Python, you can consult the following:\n",
    "\n",
    "- http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Data preparation (5 points)\n",
    "Create a data preparation and cleaning function that does the following:\n",
    "- Has a single input that is a file name string\n",
    "- Reads data (the data is comma separated, has a row header and the first column `EventID` is the index) into a pandas `dataframe`\n",
    "- Cleans the data\n",
    "  - Convert the feature `Label` to numeric (choose the minority class to be equal to 1)\n",
    "    - Create a feature `Y` with numeric label\n",
    "    - Drop the feature `Label`\n",
    "  - If a feature has missing values (i.e., `-999`): \n",
    "    - Create a dummy variable for the missing value\n",
    "      - Call the variable `orig_var_name` + `_mv` where `orig_var_name` is the name of the actual var with a missing value\n",
    "      - Give this new variable a 1 if the original variable is missing\n",
    "    - Replace the missing value with the average of the feature (make sure to compute the mean on records where the value isn't missing). You may find pandas' `.replace()` function useful.\n",
    "- After the above is done, rescales the data so that each feature has zero mean and unit variance (hint: look up sklearn.preprocessing)\n",
    "- Returns the cleaned and rescaled dataset\n",
    "\n",
    "Hint: as a guide, this function can easily be done in less than 15 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing \n",
    "import numpy as np\n",
    "\n",
    "def replace(dataset):\n",
    "    naan = -999.0                   #Missing  values\n",
    "    fields = dataset.columns.values\n",
    "    for f in fields:\n",
    "        if (sum((dataset[f] == naan)) > 0 and f !='Y'):\n",
    "            mu_f = dataset[f][(dataset[f] != naan)].mean() # mean values of the columns\n",
    "            dataset[f + \"_mv\"] = (dataset[f] == naan)*1     #adding the _mv columns for the columns with -999 values\n",
    "            dataset[f] = dataset[f].replace(to_replace=naan, value=mu_f) #replacing the -999 values with mu_f\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "def cleanBosonData(infile_name):\n",
    "    data_read = pd.read_csv(infile_name, index_col = 0) #mentioning that index is column 0\n",
    "    Y = (data_read['Label'] == 's')*1\n",
    "    data_noY = data_read.drop('Label', 1) #dropping the target variable before replacing and scaling\n",
    "    Fcall_data = replace(data_noY)\n",
    "    \n",
    "    fields = Fcall_data.columns        \n",
    "    scl = preprocessing.StandardScaler() #instance of preprocessing\n",
    "    data_clean = pd.DataFrame(scl.fit_transform(Fcall_data), columns = fields)\n",
    "    data_clean['Y'] = Y.values #adding back the target variable column to the dataset\n",
    "\n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Basic evaluations (5 points)\n",
    "In this part you will build an out-of-the box logistic regression (LR) model and support vector machine (SVM). You will then plot ROC for the LR and SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Clean the two data files included in this assignment (`data/boson_training_cut_2000.csv` and `data/boson_testing_cut.csv`) and use them as training and testing data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = cleanBosonData(\"boson_training_cut_2000.csv\")\n",
    "data_test = cleanBosonData(\"boson_testing_cut.csv\")\n",
    "#print(data_test.head())\n",
    "#print(data_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. On the training set, build the following models:\n",
    "\n",
    "- A logistic regression using sklearn's `linear_model.LogisticRegression()`. For this model, use `C=1e30`.\n",
    "- An SVM using sklearn's `svm.svc()`. For this model, specify that `kernel=\"linear\"`.\n",
    "\n",
    "For each model above, plot the ROC curve of both models on the same plot. Make sure to use the test set for computing and plotting. In the legend, also print out the Area Under the ROC (AUC) for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import linear_model,svm\n",
    "from sklearn import metrics\n",
    "\n",
    "target_var = \"Y\"\n",
    "actual = data_test[\"Y\"].copy()\n",
    "\n",
    "clf_LR = linear_model.LogisticRegression(C=1e30)\n",
    "clf_LR.fit(data_train.drop(target_var,1),data_train[target_var])\n",
    "predictions = clf_LR.predict_proba(data_test.drop(target_var,1))[:,1]\n",
    "\n",
    "clf_svm = svm.SVC(kernel='linear')\n",
    "clf_svm.fit(data_train.drop(target_var,1),data_train[target_var])\n",
    "predictions_svm = clf_svm.decision_function(data_test.drop(target_var,1))\n",
    "\n",
    "\n",
    "def plotAUC(truth, pred, lab):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(truth, pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    c = (np.random.rand(), np.random.rand(), np.random.rand())\n",
    "    plt.plot(fpr, tpr, color=c, label= lab+' (AUC = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title('ROC')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "plotAUC(actual,predictions, 'LR')\n",
    "plotAUC(actual,predictions_svm, 'SVM')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Which of the two models is generally better at ranking the test set? Are there any classification thresholds where the model identified above as \"better\" would underperform the other in a classification metric (such as TPR)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<span style=\"color:blue\">\n",
    "The logistic regression model looks better at ranking the test set. It has a higher AUC. Also, we can see on this plot that the ROC for the Logistic Regression is higher in every place. Thus, there are no thresholds where Support Vector Machines would have a higher TPR than the Logisitic Regression.\n",
    "</span>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Model selection with cross-validation (10 points)\n",
    "We think we might be able to improve the performance of the SVM if we perform a grid search on the hyper-parameter $C$.  Because we only have 1000 instances, we will have to use cross-validation to find the optimal $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Write a cross-validation function that does the following:\n",
    "- Takes as inputs a dataset, a label name, # of splits/folds (`k`), a sequence of values for $C$ (`cs`)\n",
    "- Performs two loops\n",
    "  - Outer Loop: `for each f in range(k)`:\n",
    "    - Splits the data into `data_train` & `data_validate` according to cross-validation logic\n",
    "  - Inner Loop: `for each c in cs`:\n",
    "    - Trains an SVM on training split with `C=c, kernel=\"linear\"`\n",
    "    - Computes AUC_c_k on validation data\n",
    "    - Stores AUC_c_k in a  dictionary of values\n",
    "- Returns a dictionary, where each key-value pair is: `c:[auc-c1,auc-c2,..auc-ck]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import * \n",
    "def xValSVM(dataset, label_name, k, cs):\n",
    "    try:\n",
    "        n_samples = dataset.shape[0]\n",
    "        cv = KFold(n = n_samples, n_folds = k)   #using the split method in KFold function\n",
    "        #print(cv)\n",
    "        aucs = {}\n",
    "\n",
    "        for train_index, test_index in cv:    #Collecting indices of training and test sets from kfold function\n",
    "            train_k = dataset.iloc[train_index] \n",
    "            test_k = dataset.iloc[test_index]\n",
    "            #print (test_k.head())\n",
    "\n",
    "            for c in cs:\n",
    "                svm_clf = svm.SVC(kernel = 'linear', C = c)\n",
    "                svm_clf.fit(train_k.drop(label_name, 1), train_k[label_name])\n",
    "                met = metrics.roc_auc_score(test_k[label_name], svm_clf.decision_function(test_k.drop(label_name,1)))\n",
    "\n",
    "                if c in aucs:\n",
    "                    aucs[c].append(met) #appending the auc scores for each iteration\n",
    "                else:\n",
    "                    aucs[c] = [met]\n",
    "    except:\n",
    "        print(\"----------Error in xValSVM-------------------\")\n",
    "        \n",
    "    return aucs\n",
    "    print(aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Using the function written above, do the following:\n",
    "- Generate a sequence of 10 $C$ values in the interval `[10^(-8), ..., 10^1]` (i.e., do all powers of 10 from -8 to 1).\n",
    "2.\tCall aucs = xValSVM(train, â€˜Yâ€™, 10, cs)\n",
    "3.\tFor each c in cs, get mean(AUC) and StdErr(AUC) \n",
    "4.\tCompute the value for max(meanAUC-StdErr(AUC)) across all values of c.\n",
    "5.\tGenerate a plot with the following:\n",
    "a.\tLog10(c) on the x-axis\n",
    "b.\t1 series with mean(AUC) for each c\n",
    "c.\t1 series with mean(AUC)-stderr(AUC) for each c (use â€˜k+â€™ as color pattern)\n",
    "d.\t1 series with mean(AUC)+stderr(AUC) for each c (use â€˜k--â€˜ as color pattern)\n",
    "e.\ta reference line for max(AUC-StdErr(AUC)) (use â€˜râ€™ as color pattern)\n",
    "\n",
    "Then answer the question: Did the model parameters selected beat the out-of-the-box model for SVM? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_values = {'val':[], 'mean':[], 'stderr':[]}\n",
    "folds = 10\n",
    "powers = range(-8,2)\n",
    "cs = [10**i for i in powers]\n",
    "aucs_sv = xValSVM(data_train,\"Y\",folds,cs)\n",
    "\n",
    "for i in powers:\n",
    "    dict_values['val'].append(i)                                             #value of powers of C\n",
    "    dict_values['mean'].append(np.array(aucs_sv[10**i]).mean())              #means\n",
    "    dict_values['stderr'].append(np.sqrt(np.array(aucs_sv[10**i]).var()/10)) #standard errors\n",
    "    \n",
    "means = np.array(dict_values['mean'])\n",
    "cs = np.array(dict_values['val'])       #converting all the values to Numpy arrays\n",
    "stderr = np.array(dict_values['stderr'])\n",
    "\n",
    "max_val = (means-stderr).max() #finding max(AUC-StdErr(AUC))\n",
    "\n",
    "#c_val_for_bootfunc = np.array(cs)[(means>max_val)].min() - will be 0.1\n",
    "\n",
    "plt.plot(dict_values['val'], means, label = 'means')\n",
    "plt.plot(dict_values['val'], means+stderr, 'k+-', label = 'means+stderr')\n",
    "plt.plot(dict_values['val'], means-stderr, 'k--', label = 'means-stderr')\n",
    "plt.plot(dict_values['val'], max_val*np.ones(len(means)), 'r', label = 'reference line') #for the reference line\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.xlabel('Log10(c)')\n",
    "plt.ylabel('roc_auc_score')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<span style=\"color:blue\">\n",
    "Model parameter choices selected don't beat the out-of-the-box model for SVM. The value we got defaultly is a good one.\n",
    "</span>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 4: Learning Curve with Bootstrapping\n",
    "\n",
    "In this HW we are trying to find the best linear model to predict if a record represents the Higgs Boson. One of the drivers of the performance of a model is the sample size of the training set.  As a data scientist, sometimes you have to decide if you have enough data or if you should invest in more.  We can use learning curve analysis to determine if we have reached a performance plateau. This will inform us on whether or not we should invest in more data (in this case it would be by running more experiments).\n",
    "\n",
    "Given a training set of size $N$, we test the performance of a model trained on a subsample of size $N_i$, where $N_i<=N$.  We can plot how performance grows as we move $N_i$ from $0$ to $N$.  \n",
    "\n",
    "Because of the inherent randomness of subsamples of size $N_i$, we should expect that any single sample of size $N_i$ might not be representative of an algorithmâ€™s performance at a given training set size. To quantify this variance and get a better generalization, we will also use bootstrap analysis. In bootstrap analysis, we pull multiple samples of size $N_i$, build a model, evaluate on a test set, and then take an average and standard error of the results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Create a bootstrap function that can do the following:\n",
    "\n",
    "def modBootstrapper(train, test, nruns, sampsize, lr, c):\n",
    "\n",
    "-\tTakes as input:\n",
    "    -\tA master training file (train)\n",
    "    -\tA master testing file (test)\n",
    "    -\tNumber of bootstrap iterations (nruns) \n",
    "    -\tSize of a bootstrap sample (sampsize)\n",
    "    -\tAn indicator variable to specific LR or SVM (lr=1)\n",
    "    -\tA c option (only applicable to SVM)\n",
    "\n",
    "-\tRuns a loop with (nruns) iterations, and within each loop:\n",
    "    -\tSample (sampsize) instances from train, with replacement\n",
    "    -\tFit either an SVM or LR (depending on options specified). For SVM, use the value of C identified using the 1 standard error method from part 3.    \n",
    "    -\tComputes AUC on test data using predictions from model in above step\n",
    "    -\tStores the AUC in a list\n",
    "\n",
    "-\tReturns the mean(AUC) and Standard Error(mean(AUC)) across all bootstrap samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modBootstrapper(train, test, nruns, sampsize, lr, c):\n",
    "    target = 'Y'\n",
    "    aucs_boot = []\n",
    "    for i in range(nruns):\n",
    "        train_samp = train.iloc[np.random.randint(0, len(train), size = sampsize)] #selecting random indexes for KFold\n",
    "        if (lr == 1):\n",
    "            lr_i = linear_model.LogisticRegression(C = 1e30)\n",
    "            lr_i.fit(train_samp.drop(target,1), train_samp[target]) #Linear regression fitting and predicting if lr==1\n",
    "            p = lr_i.predict_proba(test.drop(target,1))[:,1]\n",
    "        else:\n",
    "            svm_i = svm.SVC(kernel='linear', C = 0.1) #hardcoding the value since it is negative power error.\n",
    "            svm_i.fit(train_samp.drop(target,1), train_samp[target])#SVM fitting and predicting if lr==0\n",
    "            p = svm_i.decision_function(test.drop(target,1))\n",
    "        \n",
    "        aucs_boot.append(metrics.roc_auc_score(test[target], p)) #calculating auc scores for each bag in bootstrapping\n",
    "    \n",
    "    return [np.mean(aucs_boot), np.sqrt(np.var(aucs_boot))] #mean, standard error = square root of variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. For both LR and SVM, run 20 bootstrap samples for each samplesize in the following list: samplesizes = [50, 100, 200, 500, 1000, 1500, 2000]. (Note, this might take 10-15 mins â€¦ feel free to go grab a drink or watch Youtube while this runs).\n",
    "\n",
    "Generate a plot with the following:\n",
    "-\tLog2(samplesize) on the x-axis\n",
    "-\t2 sets of results lines, one for LR and one for SVM, the set should include\n",
    "    -\t1 series with mean(AUC) for each sampsize (use the color options â€˜gâ€™ for svm, â€˜râ€™ for lr)\n",
    "    -\t1 series with mean(AUC)-stderr(AUC) for each c (use â€˜+â€™ as color pattern, â€˜gâ€™,â€™râ€™ for SVM, LR respectively)\n",
    "    -\t1 series with mean(AUC)+stderr(AUC) for each c (use â€˜--â€˜ as color pattern â€˜gâ€™,â€™râ€™ for SVM, LR respectively)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SampleSizes = [50, 100, 200, 500, 1000, 1500, 2000]\n",
    "\n",
    "LR_means = []\n",
    "Lr_stderr = []\n",
    "svm_means = []\n",
    "svm_stderr = []\n",
    "for n in SampleSizes:\n",
    "    mean, err = modBootstrapper(data_train, data_test, 20, n, 1, 0.1)# collecting means and stderrs for LR model\n",
    "    LR_means.append(mean)\n",
    "    Lr_stderr.append(err)\n",
    "    mean2, err2 = modBootstrapper(data_train, data_test, 20, n, 0, 0.1)# collecting means and stderrs for SVM model\n",
    "    svm_means.append(mean2)\n",
    "    svm_stderr.append(err2)\n",
    "    \n",
    "plt.plot(np.log2(SampleSizes), LR_means, 'r', label = 'LR means')\n",
    "plt.plot(np.log2(SampleSizes), LR_means + np.array(Lr_stderr), 'r+-' , label = 'LR means + stderr')\n",
    "plt.plot(np.log2(SampleSizes), LR_means - np.array(Lr_stderr), 'r--',  label = 'LR means + stderr')\n",
    "\n",
    "plt.plot(np.log2(SampleSizes), svm_means, 'g', label = 'SVM means')\n",
    "plt.plot(np.log2(SampleSizes), svm_means + np.array(svm_stderr), 'g+-', label = 'SVM means + stderr')\n",
    "plt.plot(np.log2(SampleSizes), svm_means - np.array(svm_stderr), 'g--', label = 'SVM means - stderr')\n",
    "\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.xlabel('Log2(Sample Sizes)')\n",
    "plt.ylabel('roc_auc_score')\n",
    "\n",
    "\n",
    "#Even though it says Runtime error: Overflow encountered, please give sometime for the plot to be shown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Which of the two algorithms are more suitable for smaller sample sizes, given the set of features? If it costs twice the investment to run enough experiments to double the data, do you think it is a worthy investment?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<span style=\"color:blue\">\n",
    "\n",
    "We can see that SVM is giving the best value for smallest sample size. But it is not the same case for all the smaller sample sizes. Logistic regression does better for most of the sample sizes. \n",
    "</span>\n",
    "</p>\n",
    "<p>\n",
    "<span style=\"color:blue\">\n",
    "The performance reaches maximum point for both the algorithms. So doubling the data will likely not add anything to the performance of the algorithms. Also we don't know increase in auc would be worth interms of dollars.So we can't clearly say if it is a worthy investment.\n",
    "</span>\n",
    "</p>\n",
    "<p>\n",
    "<span style=\"color:red\">\n",
    "Note : the plot varied each time i executed because of the random indexes we are taking in modBootstrapper( ) function\n",
    "</span>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Is there a reason why cross-validation might be biased? If so, in what direction is it biased?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<span style=\"color:blue\">\n",
    "If k is number of folds, Cross-validation only uses (k-1)/k% of the data for training. Because of using subset of the actual data, we get slightly bias in cross validation. If the learning curve wonâ€™t reach the plateau at (k-1)/k% of the data, then the cross-validation procedure will be negatively biased due to the smaller training sample sizes.\n",
    "<span>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#references :\n",
    "# 2.2 : http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html,\n",
    "# 2.2 : https://datamize.wordpress.com/2015/01/24/how-to-plot-a-roc-curve-in-scikit-learn/\n",
    "# Part 3: http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-and-model-selection\n",
    "#      : https://www.youtube.com/watch?v=Gol_qOgRqfA\n",
    "# 4.1 & 4.2 : # took most of the code from  \"Lecture_ERM_LogReg_3_6053_fall17.ipynb\" notebook discussed in class.\n",
    "# 4.3 : https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/\n",
    "# 4.4 :  https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
