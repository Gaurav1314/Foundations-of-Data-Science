{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science\n",
    "## Homework 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: Disha Umarwani\n",
    "\n",
    "Student Netid: dhu20\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will be looking at data generated by particle physicists to test whether machine learning can help classify whether certain particle decay experiments identify the presence of a Higgs Boson. One does not need to know anything about particle physics to do well here, but if you are curious, full feature and data descriptions can be found here:\n",
    "\n",
    "- https://www.kaggle.com/c/higgs-boson/data\n",
    "- http://higgsml.lal.in2p3.fr/files/2014/04/documentation_v1.8.pdf\n",
    "\n",
    "The goal of this assignment is to learn to use cross-validation for model selection as well as bootstrapping for error estimation. Weâ€™ll also use learning curve analysis to understand how well different algorithms make use of limited data. For more documentation on cross-validation with Python, you can consult the following:\n",
    "\n",
    "- http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Data preparation (5 points)\n",
    "Create a data preparation and cleaning function that does the following:\n",
    "- Has a single input that is a file name string\n",
    "- Reads data (the data is comma separated, has a row header and the first column `EventID` is the index) into a pandas `dataframe`\n",
    "- Cleans the data\n",
    "  - Convert the feature `Label` to numeric (choose the minority class to be equal to 1)\n",
    "    - Create a feature `Y` with numeric label\n",
    "    - Drop the feature `Label`\n",
    "  - If a feature has missing values (i.e., `-999`): \n",
    "    - Create a dummy variable for the missing value\n",
    "      - Call the variable `orig_var_name` + `_mv` where `orig_var_name` is the name of the actual var with a missing value\n",
    "      - Give this new variable a 1 if the original variable is missing\n",
    "    - Replace the missing value with the average of the feature (make sure to compute the mean on records where the value isn't missing). You may find pandas' `.replace()` function useful.\n",
    "- After the above is done, rescales the data so that each feature has zero mean and unit variance (hint: look up sklearn.preprocessing)\n",
    "- Returns the cleaned and rescaled dataset\n",
    "\n",
    "Hint: as a guide, this function can easily be done in less than 15 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def cleanBosonData(infile_name):\n",
    "\n",
    "    #read the data from csv file\n",
    "    df = pd.read_csv('boson_training_cut_2000.csv', index_col=\"EventId\")\n",
    "    \n",
    "    #Count the number of times s and b occur\n",
    "    Counts = df['Label'].value_counts()\n",
    "    \n",
    "    #creat a data frame y where  with 0 the larger number and 1 with minority\n",
    "    if(Counts['s']>Counts['b']):\n",
    "        df['Label'] = df['Label'].map({'s': 0, 'b': 1})\n",
    "    else:\n",
    "        df['Label'] = df['Label'].map({'s': 1, 'b': 0})\n",
    "    df['Label'] = pd.to_numeric(df['Label'], errors='ignore')\n",
    "    \n",
    "    #drop the label feature\n",
    "    df = df.rename(columns = {'Label':'Y'})\n",
    "    \n",
    "    for column in df:\n",
    "        if -999.0 in df[column].values:\n",
    "        #create a dummy data and replace -999 with 1 and other with 0\n",
    "            df[df[column].name+'_mv'] = df[column]\n",
    "            df[df[column].name+'_mv'] = df[df[column].name+'_mv'].replace(-999.0, 1)\n",
    "            df[df[column].name+'_mv'] = df[df[column].name+'_mv'].replace(df.loc[df[column] != -999.0][column], 0)\n",
    "            #now I have a complete dataframe which has all the data, next step is mean replacement\n",
    "            x = df.loc[df[column] != -999.0][column]\n",
    "            df[column] = df[column].replace(-999.0, x.mean())\n",
    "    #now that I have  the dataframe completely ready for preprocessing, I'll preprocess it\n",
    "    \n",
    "    scale = StandardScaler()\n",
    "    df[['DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h',\n",
    "        'DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet',\n",
    "        'DER_deltar_tau_lep', 'DER_pt_tot']] =scale.fit_transform(df[['DER_mass_MMC', \n",
    "        'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h','DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet',\n",
    "        'DER_deltar_tau_lep', 'DER_pt_tot']])\n",
    "    data_clean = df\n",
    "    return data_clean\n",
    "#it got completed in 15 line!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Basic evaluations (5 points)\n",
    "In this part you will build an out-of-the box logistic regression (LR) model and support vector machine (SVM). You will then plot ROC for the LR and SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Clean the two data files included in this assignment (`data/boson_training_cut_2000.csv` and `data/boson_testing_cut.csv`) and use them as training and testing data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = cleanBosonData(\"data/boson_training_cut_2000.csv\")\n",
    "data_test = cleanBosonData(\"data/boson_testing_cut.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. On the training set, build the following models:\n",
    "\n",
    "- A logistic regression using sklearn's `linear_model.LogisticRegression()`. For this model, use `C=1e30`.\n",
    "- An SVM using sklearn's `svm.svc()`. For this model, specify that `kernel=\"linear\"`.\n",
    "\n",
    "For each model above, plot the ROC curve of both models on the same plot. Make sure to use the test set for computing and plotting. In the legend, also print out the Area Under the ROC (AUC) for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.755562764565\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-217c738158e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mpredic_logreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#Lets see how you performed!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mfprLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtprLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholdsLR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredic_logreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mroc_aucLR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr_LR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr_LR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn import linear_model,svm\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas.core import datetools\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "warnings.filterwarnings('ignore')\n",
    "import imp\n",
    "%matplotlib inline\n",
    "# Code here\n",
    "#Segregate your test and train data\n",
    "target = 'Y'\n",
    "Y_train = data_train[target].copy()\n",
    "\n",
    "X_train = data_train.drop(target,1) \n",
    "\n",
    "#Y_train = data_train[target]\n",
    "\n",
    "X_test = data_test.drop(target,1)\n",
    "\n",
    "Y_test = data_test[target]\n",
    "\n",
    "#Using Scikit-learn the SVC model is built with two easy steps.\n",
    "clf = svm.SVC(kernel=\"linear\")\n",
    "clf.fit(X_train, Y_train)\n",
    "#You better predict it more accurately!\n",
    "predic_clf = clf.decision_function(X_test)\n",
    "#fprSVC, tprSVC, thresholdsSVC = \n",
    "print(metrics.roc_auc_score(Y_test, predic_clf))\n",
    "roc_aucSVC = metrics.auc(fprSVC, tprSVC)\n",
    "\n",
    "#train a logistic regression model \n",
    "logreg = linear_model.LogisticRegression(C = 1e30)\n",
    "logreg.fit(X_train, Y_train)\n",
    "#predicting!\n",
    "predic_logreg = logreg.predict_proba(X_test)[:,1]\n",
    "#Lets see how you performed!\n",
    "fprLR, tprLR, thresholdsLR = metrics.roc_auc_score(Y_test, predic_logreg)\n",
    "roc_aucLR = metrics.auc(fpr_LR, tpr_LR)\n",
    "\n",
    "\n",
    "\n",
    "#There is nothing better than visualizing it\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positives')\n",
    "plt.xlabel('False Positives')\n",
    "ax2 = plt.gca().twinx()\n",
    "ax2.plot(fpr, thresholds, markeredgecolor='r', color='r')\n",
    "ax2.set_ylabel('Threshold',color='r')\n",
    "ax2.set_ylim([thresholds[-1],thresholds[0]])\n",
    "ax2.set_xlim([fpr[0],fpr[-1]])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Which of the two models is generally better at ranking the test set? Are there any classification thresholds where the model identified above as \"better\" would underperform the other in a classification metric (such as TPR)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression works better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Model selection with cross-validation (10 points)\n",
    "We think we might be able to improve the performance of the SVM if we perform a grid search on the hyper-parameter $C$.  Because we only have 1000 instances, we will have to use cross-validation to find the optimal $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Write a cross-validation function that does the following:\n",
    "- Takes as inputs a dataset, a label name, # of splits/folds (`k`), a sequence of values for $C$ (`cs`)\n",
    "- Performs two loops\n",
    "  - Outer Loop: `for each f in range(k)`:\n",
    "    - Splits the data into `data_train` & `data_validate` according to cross-validation logic\n",
    "  - Inner Loop: `for each c in cs`:\n",
    "    - Trains an SVM on training split with `C=c, kernel=\"linear\"`\n",
    "    - Computes AUC_c_k on validation data\n",
    "    - Stores AUC_c_k in a  dictionary of values\n",
    "- Returns a dictionary, where each key-value pair is: `c:[auc-c1,auc-c2,..auc-ck]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model,svm\n",
    "def xValSVM(dataset, label_name, k, cs):\n",
    "    aucs = dict()\n",
    "    kf = KFold(n_splits=k, shuffle = True)\n",
    "    train_indices = list()\n",
    "    test_indices = list()\n",
    "    for train_index, test_index in kf.split(dataset):\n",
    "        train_indices.append(train_index)\n",
    "        test_indices.append(test_index)\n",
    "    for f in range(k):\n",
    "        trin = list(train_indices[f])\n",
    "        tein= list(test_indices[f])\n",
    "        for c in cs:\n",
    "            SVMCrss = svm.SVC(C = c , kernel = \"linear\",probability = True)\n",
    "            SVMCrss.fit(X_train, Y_train)\n",
    "            pred = SVMCrss.predict_proba(X_test)[:,1]\n",
    "            fpr,tpr,threshold = metrics.roc_curve(Y_test, pred)\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "            aucs.setdefault(cs, [])\n",
    "            aucs[cs].append(roc_auc)\n",
    "    return aucs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Using the function written above, do the following:\n",
    "- Generate a sequence of 10 $C$ values in the interval `[10^(-8), ..., 10^1]` (i.e., do all powers of 10 from -8 to 1).\n",
    "2.\tCall aucs = xValSVM(train, â€˜Yâ€™, 10, cs)\n",
    "3.\tFor each c in cs, get mean(AUC) and StdErr(AUC) \n",
    "4.\tCompute the value for max(meanAUC-StdErr(AUC)) across all values of c.\n",
    "5.\tGenerate a plot with the following:\n",
    "a.\tLog10(c) on the x-axis\n",
    "b.\t1 series with mean(AUC) for each c\n",
    "c.\t1 series with mean(AUC)-stderr(AUC) for each c (use â€˜k+â€™ as color pattern)\n",
    "d.\t1 series with mean(AUC)+stderr(AUC) for each c (use â€˜k--â€˜ as color pattern)\n",
    "e.\ta reference line for max(AUC-StdErr(AUC)) (use â€˜râ€™ as color pattern)\n",
    "\n",
    "Then answer the question: Did the model parameters selected beat the out-of-the-box model for SVM? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d5b7dc39e6a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0maucs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxValSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mMean_AUC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mStand_err_AUC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-3125c8fa2f46>\u001b[0m in \u001b[0;36mxValSVM\u001b[0;34m(dataset, label_name, k, cs)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0maucs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0maucs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maucs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "from math import log\n",
    "C = [10**(-8),10**(-7),10**(-6),10**(-5),10**(-4),10**(-3),10**(-2),10**(-1),10**(0),10]\n",
    "aucs = xValSVM(data_train,'Y',10,C)\n",
    "Mean_AUC = list()\n",
    "Stand_err_AUC = list()\n",
    "for key in aucs:\n",
    "    Mean_AUC.append(aucs.values()[key].mean())\n",
    "    Stand_err.append(statistics.stdev(aucs[key]))\n",
    "\n",
    "Mean_AUC = np.asarray(Mean_AUC)\n",
    "Stand_err_AUC = np.asarray(Stand_err_AUC)\n",
    "sub = np.subtract(Mean_AUC, Stand_err_AUC)\n",
    "ad = np.add(Mean_AUC, Stand_err_AUC)\n",
    "npmax = np.amax(sub)\n",
    "print(\"The maximum value is\"+ npmax)\n",
    "\n",
    "#Visulaization\n",
    "from math import log10\n",
    "base = [log10(y) for y in C]\n",
    "\n",
    "plt.title(\"Grid Search for Hyper Parameter C\")\n",
    "plt.legend(loc = 'lower left')\n",
    "plt.plot(base, Mean_AUC, 'b', label = 'Mean AUC' )\n",
    "plt.plot(base, sub , 'k+', label = 'Mean(AUC) - StdErr(AUC)' )\n",
    "plt.plot(base, ad , 'k--', label = 'Mean(AUC) + StdErr(AUC)' )\n",
    "\n",
    "plt.axhline(y=npmax,xmin=base[0], xmax=base[len(base)-1], hold=None, color = 'r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Part 4: Learning Curve with Bootstrapping\n",
    "In this HW we are trying to find the best linear model to predict if a record represents the Higgs Boson. One of the drivers of the performance of a model is the sample size of the training set.  As a data scientist, sometimes you have to decide if you have enough data or if you should invest in more.  We can use learning curve analysis to determine if we have reached a performance plateau. This will inform us on whether or not we should invest in more data (in this case it would be by running more experiments).\n",
    "\n",
    "Given a training set of size $N$, we test the performance of a model trained on a subsample of size $N_i$, where $N_i<=N$.  We can plot how performance grows as we move $N_i$ from $0$ to $N$.  \n",
    "\n",
    "Because of the inherent randomness of subsamples of size $N_i$, we should expect that any single sample of size $N_i$ might not be representative of an algorithmâ€™s performance at a given training set size. To quantify this variance and get a better generalization, we will also use bootstrap analysis. In bootstrap analysis, we pull multiple samples of size $N_i$, build a model, evaluate on a test set, and then take an average and standard error of the results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Create a bootstrap function that can do the following:\n",
    "\n",
    "def modBootstrapper(train, test, nruns, sampsize, lr, c):\n",
    "\n",
    "-\tTakes as input:\n",
    "    -\tA master training file (train)\n",
    "    -\tA master testing file (test)\n",
    "    -\tNumber of bootstrap iterations (nruns) \n",
    "    -\tSize of a bootstrap sample (sampsize)\n",
    "    -\tAn indicator variable to specific LR or SVM (lr=1)\n",
    "    -\tA c option (only applicable to SVM)\n",
    "\n",
    "-\tRuns a loop with (nruns) iterations, and within each loop:\n",
    "    -\tSample (sampsize) instances from train, with replacement\n",
    "    -\tFit either an SVM or LR (depending on options specified). For SVM, use the value of C identified using the 1 standard error method from part 3.    \n",
    "    -\tComputes AUC on test data using predictions from model in above step\n",
    "    -\tStores the AUC in a list\n",
    "\n",
    "-\tReturns the mean(AUC) and Standard Error(mean(AUC)) across all bootstrap samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "def modBootstrapper(train, test, nruns, sampsize, lr, c):\n",
    "    aucs = []\n",
    "    target = 'Label'\n",
    "    for i in range(nruns):\n",
    "        train_samp = train.iloc[np.random.randint(0, len(train), size=sampsize)]\n",
    "        if lr == 1:\n",
    "            logreg = linear_model.LogisticRegression(C = 1e30)\n",
    "            logreg.fit(train_samp.drop(target,1), train_samp[target])\n",
    "            aucs[i] = roc_auc_score(test.drop(target,1), clf.predict(test[target]))\n",
    "        else:\n",
    "            clf = SVC(kernel='linear', C=c)\n",
    "            probas = clf.fit(train.drop(target,1), train[target])\n",
    "            aucs[i] = roc_auc_score(test.drop(target,1), clf.predict(test[target]))\n",
    "    aucs_res = []\n",
    "    aucs_res.append(np.mean(aucs))\n",
    "    aucs_res.append(np.std(np.mean(aucs)))\n",
    "    return aucs_res\n",
    "\n",
    "\n",
    "def modBootstrapper(train, test, nruns, sampsize, lr, c):\n",
    "    aucs_res = []\n",
    "    test_features = test.iloc[:,:-1]\n",
    "    test_labels = test.iloc[:,-1]\n",
    "    print(train.shape)\n",
    "    print(test.shape)\n",
    "    for i in range(nruns):\n",
    "        train_samp = train.iloc[np.random.randint(0, len(train), size=sampsize)]\n",
    "        train_features = train_samp.iloc[:,:-1]\n",
    "        train_labels = train_samp.iloc[:,-1]\n",
    "        if lr == 0 :\n",
    "            logisticRegression = linear_model.LogisticRegression(C=1e30)\n",
    "            logisticRegression.fit(train_features,train_labels)\n",
    "            ans = metrics.roc_auc_score(test_labels, logisticRegression.predict_proba(test_features)[:,1])\n",
    "            aucs_res.append(ans)\n",
    "        elif lr == 1:\n",
    "            svm_cross_validation = svm.SVC(C = c , kernel = \"linear\",probability = True)\n",
    "            svm_cross_validation.fit(train_features,train_labels)\n",
    "            ans = metrics.roc_auc_score(test_labels, svm_cross_validation.predict_proba(test_features)[:,1])\n",
    "            aucs_res.append(ans)\n",
    "    print(aucs_res)\n",
    "    return aucs_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. For both LR and SVM, run 20 bootstrap samples for each samplesize in the following list: samplesizes = [50, 100, 200, 500, 1000, 1500, 2000]. (Note, this might take 10-15 mins â€¦ feel free to go grab a drink or watch Youtube while this runs).\n",
    "\n",
    "Generate a plot with the following:\n",
    "-\tLog2(samplesize) on the x-axis\n",
    "-\t2 sets of results lines, one for LR and one for SVM, the set should include\n",
    "    -\t1 series with mean(AUC) for each sampsize (use the color options â€˜gâ€™ for svm, â€˜râ€™ for lr)\n",
    "    -\t1 series with mean(AUC)-stderr(AUC) for each c (use â€˜+â€™ as color pattern, â€˜gâ€™,â€™râ€™ for SVM, LR respectively)\n",
    "    -\t1 series with mean(AUC)+stderr(AUC) for each c (use â€˜--â€˜ as color pattern â€˜gâ€™,â€™râ€™ for SVM, LR respectively)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code here\n",
    "samplesizes = [50, 100, 200, 500, 1000, 1500, 2000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Which of the two algorithms are more suitable for smaller sample sizes, given the set of features? If it costs twice the investment to run enough experiments to double the data, do you think it is a worthy investment?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Is there a reason why cross-validation might be biased? If so, in what direction is it biased?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
